{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "import zipapp\n",
    "import requests\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_per_section = 50\n",
    "skip = int(len_per_section / 2)\n",
    "batch_size = 50\n",
    "max_steps = 1000000\n",
    "log_every = 500\n",
    "learning_rate = 10.\n",
    "\n",
    "dataset_source = 'https://tools.sofora.net/uploads/mini-shakespeare.txt'\n",
    "send_backup_url='https://tools.sofora.net/index.php'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = requests.get(dataset_source).text\n",
    "\n",
    "text_len = len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(set(text))\n",
    "char_size = len(chars)\n",
    "\n",
    "char2id = dict((c, i) for i, c in enumerate(chars))\n",
    "id2char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suggested model Nh = 2/3 * (Ni + No) from \n",
    "# https://towardsdatascience.com/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046\n",
    "hidden_nodes = int(2/3 * (len_per_section * char_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, text_len - len_per_section, skip):\n",
    "    sections.append(text[i: i + len_per_section])\n",
    "    next_chars.append(text[i + len_per_section])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(sections), len_per_section, char_size), dtype=int)\n",
    "y = np.zeros((len(sections), char_size), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, section in enumerate(sections):\n",
    "    for j, char in enumerate(section):\n",
    "        X[i, j, char2id[char]] = 1.\n",
    "    y[i, char2id[next_chars[i]]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm(i, o, s):\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, w_ii) + tf.matmul(o, w_io) + b_i)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, w_fi) + tf.matmul(o, w_fo) + b_f)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, w_oi) + tf.matmul(o, w_oo) + b_o)\n",
    "    memory_cell = tf.sigmoid(tf.matmul(i, w_ci) + tf.matmul(o, w_co) + b_c)\n",
    "\n",
    "    s = forget_gate * s + input_gate * memory_cell\n",
    "    o = output_gate * tf.tanh(s)\n",
    "\n",
    "    return o, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    global_step = tf.Variable(0)\n",
    "    \n",
    "    data = tf.placeholder(tf.float32, [batch_size, len_per_section, char_size])\n",
    "    \n",
    "    labels = tf.placeholder(tf.float32, [batch_size, char_size])\n",
    "    \n",
    "    w_ii = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1, tf.float32))\n",
    "    w_io = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1, tf.float32))\n",
    "    b_i = tf.Variable(tf.zeros([1, hidden_nodes], tf.float32))\n",
    "\n",
    "    w_fi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1, tf.float32))\n",
    "    w_fo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1, tf.float32))\n",
    "    b_f = tf.Variable(tf.zeros([1, hidden_nodes], tf.float32))\n",
    "\n",
    "    w_oi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1, tf.float32))\n",
    "    w_oo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1, tf.float32))\n",
    "    b_o = tf.Variable(tf.zeros([1, hidden_nodes], tf.float32))\n",
    "\n",
    "    w_ci = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1, tf.float32))\n",
    "    w_co = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1, tf.float32))\n",
    "    b_c = tf.Variable(tf.zeros([1, hidden_nodes], tf.float32))\n",
    "    \n",
    "    output = tf.zeros([batch_size, hidden_nodes])\n",
    "    state = tf.zeros([batch_size, hidden_nodes])\n",
    "    \n",
    "    for i in range(len_per_section):\n",
    "\n",
    "        output, state = lstm(data[:, i, :], output, state)\n",
    "\n",
    "        if i == 0:  # if first section\n",
    "            outputs_all_i = output  # make current output the start\n",
    "            labels_all_i = data[:, i + 1, :]  # make next input as the start\n",
    "\n",
    "        elif i != len_per_section - 1:  # not first or last section\n",
    "            outputs_all_i = tf.concat([outputs_all_i, output], 0)  # append the current output\n",
    "            labels_all_i = tf.concat([labels_all_i, data[:, i + 1, :]], 0)  # append the next input\n",
    "\n",
    "        else:  # the last section\n",
    "            outputs_all_i = tf.concat([outputs_all_i, output], 0)  # append the current output\n",
    "            labels_all_i = tf.concat([labels_all_i, labels], 0)  # append empty label\n",
    "\n",
    "    w = tf.Variable(tf.truncated_normal([hidden_nodes, char_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([char_size]))\n",
    "\n",
    "    logits = tf.matmul(outputs_all_i, w) + b\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels_all_i))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    ###########\n",
    "    # Test Graph tensors\n",
    "    ###########\n",
    "\n",
    "    test_data = tf.placeholder(tf.float32, shape=[1, char_size])\n",
    "\n",
    "    test_output = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "\n",
    "    test_state = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "\n",
    "    reset_test_state = tf.group(\n",
    "        test_output.assign(tf.zeros([1, hidden_nodes])),\n",
    "        test_state.assign(tf.zeros([1, hidden_nodes]))\n",
    "    )\n",
    "\n",
    "    test_output, test_state = lstm(test_data, test_output, test_state)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_output, w) + b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_directory = 'logs'\n",
    "\n",
    "\n",
    "class Helper:\n",
    "\n",
    "    def __init__(self, send_backup_url=None):\n",
    "        if tf.gfile.Exists(logs_directory) is False:\n",
    "            tf.gfile.MakeDirs(logs_directory)\n",
    "\n",
    "        self.url = send_backup_url\n",
    "        self.dir_count = str(len(tf.gfile.ListDirectory(logs_directory)))\n",
    "\n",
    "        self.send_zip = 'send_' + self.dir_count + '.zip'\n",
    "\n",
    "        self.logs_directory = logs_directory + \"/\" + self.dir_count\n",
    "        self.model_dir = self.logs_directory + \"/model\"\n",
    "        self.output_file = self.logs_directory + \"/__main__.py\"\n",
    "\n",
    "        tf.gfile.MakeDirs(self.model_dir)\n",
    "\n",
    "    def backup(self):\n",
    "        \"\"\"\n",
    "        After every model saving, text generated, loss, step, and time log is saved.\n",
    "        Also with the model. All is zipped and send online for backup.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.url is not None:\n",
    "\n",
    "            # zip backup folder\n",
    "            zipapp.create_archive(self.logs_directory, self.send_zip)\n",
    "\n",
    "            # then send zipped folder to the URL\n",
    "            try:\n",
    "                requests.post(self.url, files={\n",
    "                    'uploaded_file': (os.path.basename(self.send_zip), open(self.send_zip, 'rb')),\n",
    "                })\n",
    "            except requests.exceptions.ConnectionError as error:\n",
    "                print(error)\n",
    "\n",
    "    def get_ckpt_dir(self):\n",
    "        return self.model_dir + \"/model\"\n",
    "\n",
    "    def write_file(self, _content, print_it=False):\n",
    "        if print_it:\n",
    "            print(_content)\n",
    "        _file = open(self.output_file, 'a')\n",
    "        _file.write(_content)\n",
    "        _file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "helper = Helper(send_backup_url=send_backup_url)\n",
    "\n",
    "helper.write_file(\"Step,Training Lost,Timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "9,1.8251204,2019-05-14 21:53:22.626456\n",
      "\n",
      "19,1.4272484,2019-05-14 21:53:23.820357\n",
      "\n",
      "29,1.3443614,2019-05-14 21:53:24.852348\n",
      "\n",
      "39,1.2068406,2019-05-14 21:53:25.535143\n",
      "\n",
      "49,1.1788019,2019-05-14 21:53:26.240352\n",
      "\n",
      "59,1.2512608,2019-05-14 21:53:27.226339\n",
      "\n",
      "69,1.6366446,2019-05-14 21:53:27.947635\n",
      "\n",
      "79,0.9891515,2019-05-14 21:53:28.674813\n",
      "\n",
      "89,1.1715182,2019-05-14 21:53:29.501090\n",
      "\n",
      "99,1.360497,2019-05-14 21:53:30.409645\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    offset = 0\n",
    "    saver = tf.train.Saver()\n",
    "    X_length = len(X)\n",
    "\n",
    "    # make steps\n",
    "    for step in range(max_steps):\n",
    "\n",
    "        offset = offset % X_length\n",
    "\n",
    "        if offset <= (X_length - batch_size):\n",
    "            batch_data = X[offset: offset + batch_size]\n",
    "            batch_labels = y[offset: offset + batch_size]\n",
    "            offset += batch_size\n",
    "        else:\n",
    "            to_add = batch_size - (X_length - offset)\n",
    "            batch_data = np.concatenate((X[offset:X_length], X[0: to_add]))\n",
    "            batch_labels = np.concatenate((y[offset:X_length], y[0: to_add]))\n",
    "            offset = to_add\n",
    "\n",
    "        _, training_loss = sess.run([optimizer, loss], feed_dict={data: batch_data, labels: batch_labels})\n",
    "\n",
    "        if (step + 1) % log_every == 0:\n",
    "            saver.save(sess, helper.get_ckpt_dir(), global_step=step)\n",
    "            \n",
    "            helper.write_file(\n",
    "                \"\\n\" + str(step) +\n",
    "                \",\" + str(training_loss) +\n",
    "                \",\" + str(datetime.datetime.now()),\n",
    "                print_it=True\n",
    "            )\n",
    "\n",
    "            helper.backup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
